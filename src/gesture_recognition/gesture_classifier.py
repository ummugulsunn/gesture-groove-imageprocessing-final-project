"""
Gesture Classifier Module
Hand gesture classification system
"""

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
from typing import List, Optional, Tuple
import os
import time


class GestureClassifier:
    """
    Hand gesture classification system
    Features to be developed:
    - Real-time gesture recognition
    - Advanced feature extraction
    - Machine learning model integration
    - Performance optimization
    """
    
    def __init__(self, model_path: str = "models/gesture_model.pkl"):
        """
        Initializes GestureClassifier
        
        Args:
            model_path: Trained model file path
        """
        self.model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10,
            min_samples_split=5
        )
        
        self.model_path = model_path
        self.is_trained = False
        
        # Gesture classes
        self.gesture_classes = [
            'open_hand',    # Open hand
            'fist',         # Fist
            'point_up',     # Point up
            'point_down',   # Point down
            'peace'         # Peace sign
        ]
        
        # Feature names for debugging
        self.feature_names = []
        self._generate_feature_names()
        
        # Prediction smoothing
        self.prediction_history = []
        self.smoothing_window = 5
        
        # Load model if exists
        self.load_model()
        
    def _generate_feature_names(self):
        """Generates feature names for the classifier"""
        # Raw landmarks (21 * 3 = 63)
        for i in range(21):
            self.feature_names.extend([f'landmark_{i}_x', f'landmark_{i}_y', f'landmark_{i}_z'])
        
        # Finger distances (5)
        for finger in ['thumb', 'index', 'middle', 'ring', 'pinky']:
            self.feature_names.append(f'{finger}_tip_distance')
        
        # Finger angles (5)
        for finger in ['thumb', 'index', 'middle', 'ring', 'pinky']:
            self.feature_names.append(f'{finger}_angle')
        
        # Hand shape features (3)
        self.feature_names.extend(['hand_width', 'hand_height', 'hand_area'])
        
    def extract_advanced_features(self, landmarks: np.ndarray) -> np.ndarray:
        """
        Extracts advanced features from hand landmarks
        Features:
        - Finger angles
        - Hand orientation
        - Gesture dynamics
        """
        if landmarks is None or len(landmarks) != 63:
            return np.zeros(76)  # 63 + 13 additional features
            
        # Reshape landmarks to 3D coordinates
        points = landmarks.reshape(21, 3)
        
        features = list(landmarks)  # Raw landmarks (63)
        
        # Finger tip distances (relative to palm center)
        wrist = points[0]  # Wrist point
        
        # Finger tip indices (MediaPipe standard)
        tip_indices = [4, 8, 12, 16, 20]  # thumb, index, middle, ring, pinky
        
        for tip_idx in tip_indices:
            tip_distance = np.linalg.norm(points[tip_idx] - wrist)
            features.append(tip_distance)
        
        # Finger angles (TODO: To be implemented by AyÅŸenur)
        for i in range(5):
            features.append(0.0)  # Placeholder
        
        # Hand shape features
        x_coords = points[:, 0]
        y_coords = points[:, 1]
        
        hand_width = np.max(x_coords) - np.min(x_coords)
        hand_height = np.max(y_coords) - np.min(y_coords)
        hand_area = hand_width * hand_height
        
        features.extend([hand_width, hand_height, hand_area])
        
        return np.array(features)
    
    def calculate_finger_angles(self, landmarks: np.ndarray) -> List[float]:
        """Calculates angles between finger joints"""
        if landmarks is None or len(landmarks) != 63:
            return [0.0] * 5
            
        points = landmarks.reshape(21, 3)
        angles = []
        
        # Finger joint indices
        finger_connections = [
            [1, 2, 3, 4],     # Thumb
            [5, 6, 7, 8],     # Index
            [9, 10, 11, 12],  # Middle
            [13, 14, 15, 16], # Ring
            [17, 18, 19, 20]  # Pinky
        ]
        
        for finger_joints in finger_connections:
            try:
                # Use middle joint as reference for each finger
                if len(finger_joints) >= 3:
                    p1 = points[finger_joints[0]]  # MCP (base)
                    p2 = points[finger_joints[1]]  # PIP (middle)
                    p3 = points[finger_joints[2]]  # DIP (tip)
                    
                    # Create two vectors
                    v1 = p1 - p2  # From PIP to MCP
                    v2 = p3 - p2  # From PIP to DIP
                    
                    # Calculate angle (cosine formula)
                    dot_product = np.dot(v1, v2)
                    norms = np.linalg.norm(v1) * np.linalg.norm(v2)
                    
                    if norms > 0:
                        cos_angle = np.clip(dot_product / norms, -1.0, 1.0)
                        angle_rad = np.arccos(cos_angle)
                        angle_deg = np.degrees(angle_rad)
                        angles.append(angle_deg)
                    else:
                        angles.append(0.0)
                else:
                    angles.append(0.0)
            except Exception:
                angles.append(0.0)
        
        return angles
    
    def collect_training_data(self, landmarks: np.ndarray, gesture: str):
        """
        Collects training data for gesture recognition
        Saves landmarks and labels for model training
        
        Args:
            landmarks: Hand landmarks array
            gesture: Gesture class label
        """
        if landmarks is None or gesture not in self.gesture_classes:
            return False
            
        # Create data directory if not exists
        data_dir = "data/training"
        os.makedirs(data_dir, exist_ok=True)
        
        # Save raw landmarks
        timestamp = int(time.time())
        filename = f"{data_dir}/{gesture}_{timestamp}.npy"
        
        try:
            np.save(filename, landmarks)
            print(f"âœ… Training data saved: {gesture}")
            return True
        except Exception as e:
            print(f"âŒ Error saving training data: {e}")
            return False
    
    def _augment_and_save(self, original_data: dict, data_dir: str):
        """
        AYÅENUR'UN GELÄ°ÅTÄ°RECEÄÄ° Ã–ZELLÄ°K:
        Data augmentation teknikleri
        
        Techniques to implement:
        1. Rotation augmentation
        2. Scale variations
        3. Translation shifts
        4. Noise injection
        5. Temporal variations
        """
        import json
        
        gesture = original_data['gesture']
        base_landmarks = np.array(original_data['raw_landmarks'])
        
        # 1. Scale variations (Â±10%)
        for scale in [0.9, 1.1]:
            scaled_landmarks = base_landmarks * scale
            augmented_sample = original_data.copy()
            augmented_sample['raw_landmarks'] = scaled_landmarks.tolist()
            augmented_sample['features'] = self.extract_advanced_features(scaled_landmarks).tolist()
            augmented_sample['augmentation'] = f'scale_{scale}'
            
            timestamp = int(time.time() * 1000)
            filename = f"{data_dir}/{gesture}_{timestamp}_aug_scale.json"
            with open(filename, 'w') as f:
                json.dump(augmented_sample, f, indent=2)
        
        # 2. Noise injection (Â±2% gaussian noise)
        for noise_level in [0.02]:
            noise = np.random.normal(0, noise_level, base_landmarks.shape)
            noisy_landmarks = base_landmarks + noise
            
            augmented_sample = original_data.copy()
            augmented_sample['raw_landmarks'] = noisy_landmarks.tolist()
            augmented_sample['features'] = self.extract_advanced_features(noisy_landmarks).tolist()
            augmented_sample['augmentation'] = f'noise_{noise_level}'
            
            timestamp = int(time.time() * 1000)
            filename = f"{data_dir}/{gesture}_{timestamp}_aug_noise.json"
            with open(filename, 'w') as f:
                json.dump(augmented_sample, f, indent=2)
        
        print(f"ğŸ“ˆ Data augmentation tamamlandÄ±: {gesture}")
    
    def load_training_dataset(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        AYÅENUR'UN GELÄ°ÅTÄ°RECEÄÄ° Ã–ZELLÄ°K:
        Toplanan eÄŸitim verisini yÃ¼kle
        
        Returns:
            Tuple[X_features, y_labels]
        """
        data_dir = "data/training_samples"
        
        if not os.path.exists(data_dir):
            print(f"âŒ Training data directory bulunamadÄ±: {data_dir}")
            return np.array([]), np.array([])
        
        X_data = []
        y_data = []
        
        import json
        
        # JSON dosyalarÄ±nÄ± yÃ¼kle
        for filename in os.listdir(data_dir):
            if filename.endswith('.json'):
                try:
                    with open(os.path.join(data_dir, filename), 'r') as f:
                        sample = json.load(f)
                    
                    features = np.array(sample['features'])
                    gesture = sample['gesture']
                    
                    if len(features) > 0 and gesture in self.gesture_classes:
                        X_data.append(features)
                        y_data.append(gesture)
                        
                except Exception as e:
                    print(f"âš ï¸ Sample yÃ¼kleme hatasÄ± {filename}: {e}")
        
        if len(X_data) == 0:
            print("âŒ HiÃ§ training sample bulunamadÄ±!")
            return np.array([]), np.array([])
        
        X = np.array(X_data)
        y = np.array(y_data)
        
        print(f"ğŸ“Š Dataset yÃ¼klendi: {len(X)} sample, {len(np.unique(y))} class")
        return X, y
    
    def evaluate_model_performance(self, X_test: np.ndarray, y_test: np.ndarray):
        """
        Evaluates model performance
        Calculates accuracy and other metrics
        """
        if not self.is_trained:
            print("âŒ Model henÃ¼z eÄŸitilmedi!")
            return
        
        from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
        
        # Predictions
        y_pred = self.model.predict(X_test)
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred, labels=self.gesture_classes)
        
        print("\nğŸ“Š PERFORMANS ANALÄ°ZÄ°")
        print("=" * 50)
        
        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            y_test, y_pred, labels=self.gesture_classes, average=None
        )
        
        print(f"{'Gesture':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<8}")
        print("-" * 50)
        
        for i, gesture in enumerate(self.gesture_classes):
            print(f"{gesture:<12} {precision[i]:<10.3f} {recall[i]:<10.3f} {f1[i]:<10.3f} {support[i]:<8}")
        
        # Overall metrics
        avg_precision = np.mean(precision)
        avg_recall = np.mean(recall)
        avg_f1 = np.mean(f1)
        
        print("-" * 50)
        print(f"{'AVERAGE':<12} {avg_precision:<10.3f} {avg_recall:<10.3f} {avg_f1:<10.3f}")
        
        # Feature importance
        print("\nğŸ“ˆ EN Ã–NEMLÄ° Ã–ZELLÄ°KLER")
        print("-" * 30)
        
        feature_importance = self.model.feature_importances_
        if len(self.feature_names) == len(feature_importance):
            important_features = sorted(
                zip(self.feature_names, feature_importance),
                key=lambda x: x[1], reverse=True
            )[:15]
            
            for i, (feat_name, importance) in enumerate(important_features, 1):
                print(f"{i:2d}. {feat_name:<25} {importance:.4f}")
        
        return {
            'confusion_matrix': cm,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'feature_importance': feature_importance
        }
    
    def train_model(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, str]:
        """
        Trains the gesture recognition model
        Uses collected training data
        
        Args:
            X: Ã–zellik matrisi
            y: Etiket dizisi
            
        Returns:
            Tuple[accuracy, classification_report]
        """
        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Model eÄŸitimi
        self.model.fit(X_train, y_train)
        
        # Test performansÄ±
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=self.gesture_classes)
        
        self.is_trained = True
        
        # Feature importance analysis
        feature_importance = self.model.feature_importances_
        
        print(f"âœ… Model eÄŸitimi tamamlandÄ±!")
        print(f"ğŸ“Š Test Accuracy: {accuracy:.3f}")
        print("ğŸ“ˆ En Ã¶nemli Ã¶zellikler:")
        
        # En Ã¶nemli 10 Ã¶zelliÄŸi gÃ¶ster
        if len(self.feature_names) == len(feature_importance):
            important_features = sorted(
                zip(self.feature_names, feature_importance),
                key=lambda x: x[1], reverse=True
            )[:10]
            
            for feat_name, importance in important_features:
                print(f"   {feat_name}: {importance:.3f}")
        
        return accuracy, report
    
    def predict_gesture(self, landmarks, opencv_gesture=None):
        """
        Predicts gesture from hand landmarks
        Returns gesture class and confidence
        
        Bu implementasyon temel bir yaklaÅŸÄ±m sunar:
        1. Model varsa model prediction
        2. Yoksa rule-based classification  
        3. OpenCV gesture'Ä±nÄ± fallback olarak kullan
        """
        try:
            if landmarks is None:
                return None
                
            # OpenCV gesture priority (eÄŸer MediaPipe Ã§alÄ±ÅŸmÄ±yorsa)
            if opencv_gesture and opencv_gesture != 'unknown':
                print(f"ğŸ” OpenCV Gesture: {opencv_gesture}")
                return opencv_gesture
            
            # Ã–nce rule-based classification dene (model eÄŸitilmemiÅŸ olabilir)
            gesture = self._rule_based_classification(landmarks)
            if gesture:
                print(f"ğŸ“ Rule-based: {gesture}")
                return gesture
            
            # Model eÄŸitilmiÅŸse kullan
            if self.is_trained and hasattr(self, 'model') and self.model is not None:
                try:
                    # Feature extraction
                    features = self._extract_features(landmarks)
                    
                    # Trained model prediction
                    if not hasattr(self.model, 'classes_'):
                        print("âš ï¸ Model eÄŸitilmemiÅŸ, `predict_proba` Ã§aÄŸrÄ±lamaz.")
                        return None

                    prediction = self.model.predict([features])
                    if hasattr(self.model, 'predict_proba'):
                        confidence = max(self.model.predict_proba([features])[0])
                    else:
                        confidence = 0.9

                    gesture = prediction[0]
                    
                    if confidence > 0.6:  # Confidence threshold
                        print(f"ğŸ¯ Model Prediction: {gesture} (confidence: {confidence:.2f})")
                        return gesture
                    else:
                        print(f"ğŸ¤” Low confidence: {gesture} ({confidence:.2f})")
                except Exception as model_error:
                    print(f"âš ï¸ Model prediction hatasÄ±: {model_error}")
                    
            return None
            
        except Exception as e:
            print(f"âŒ Gesture prediction hatasÄ±: {e}")
            return None
    
    def _extract_features(self, landmarks):
        """Basit feature extraction"""
        try:
            # Bu daha geliÅŸmiÅŸ extract_advanced_features'Ä± Ã§aÄŸÄ±rÄ±r
            return self.extract_advanced_features(landmarks)
        except:
            # Fallback: basic features
            if len(landmarks) >= 21:
                return landmarks[:21].flatten()
            return landmarks.flatten()
    
    def _rule_based_classification(self, landmarks):
        """Rule-based gesture classification"""
        try:
            # Landmarks format kontrolÃ¼
            if landmarks is None:
                return None
                
            # Numpy array'e Ã§evir
            if not isinstance(landmarks, np.ndarray):
                landmarks = np.array(landmarks)
                
            # Flatten edilmiÅŸse reshape et
            if landmarks.ndim == 1:
                total_elements = len(landmarks)
                if total_elements == 63:  # 21 points * 3 coordinates
                    landmarks = landmarks.reshape(21, 3)
                elif total_elements == 42:  # 21 points * 2 coordinates
                    landmarks = landmarks.reshape(21, 2)
                elif total_elements >= 63:
                    # Fazla veriyi kes ve reshape et
                    landmarks = landmarks[:63].reshape(21, 3)
                elif total_elements >= 42:
                    # Fazla veriyi kes ve reshape et  
                    landmarks = landmarks[:42].reshape(21, 2)
                else:
                    print(f"âš ï¸ Beklenmeyen landmark boyutu: {total_elements}")
                    return None
                    
            # Minimum 21 landmark gerekli
            if len(landmarks) < 21:
                return None
                
            # Finger state detection
            fingers_up = []
            
            # Thumb kontrolÃ¼ (YENÄ° YAKLAÅIM: Kendi MCP eklemine gÃ¶re Y konumu)
            try:
                # landmarks[4] = THUMB_TIP, landmarks[2] = THUMB_MCP
                # BaÅŸparmak ucu, kendi MCP ekleminin Y koordinatÄ±ndan eÅŸik kadar yukarÄ±daysa aÃ§Ä±k kabul edilir.
                thumb_tip_y = landmarks[4][1]
                thumb_mcp_y = landmarks[2][1]
                THUMB_VERTICAL_THRESHOLD = 0.045 # EÅŸik deÄŸeri ARTIRILDI (0.040 -> 0.045)
                if thumb_tip_y < thumb_mcp_y - THUMB_VERTICAL_THRESHOLD:
                    fingers_up.append(1)  # BaÅŸparmak aÃ§Ä±k
                else:
                    fingers_up.append(0)  # BaÅŸparmak kapalÄ±
            except IndexError:
                fingers_up.append(0) # Hata durumunda kapalÄ± varsay
            except Exception as e:
                # print(f"âš ï¸ Thumb detection error: {e}") # Gerekirse loglama
                fingers_up.append(0)
                
            # DiÄŸer parmaklar (Y koordinatÄ±na gÃ¶re - vertical movement)
            # Index, Middle, Ring, Pinky
            finger_tip_indices = [8, 12, 16, 20]
            finger_mcp_indices = [5, 9, 13, 17]
            FINGER_VERTICAL_THRESHOLD = 0.050 # EÅŸik deÄŸeri DÃœÅÃœRÃœLDÃœ (0.060 -> 0.050)
            
            for tip_idx, mcp_idx in zip(finger_tip_indices, finger_mcp_indices):
                try:
                    tip_y = landmarks[tip_idx][1]
                    mcp_y = landmarks[mcp_idx][1]
                    if tip_y < mcp_y - FINGER_VERTICAL_THRESHOLD: # Tip, MCP'nin Ã¼zerindeyse aÃ§Ä±k
                        fingers_up.append(1)
                    else:
                        fingers_up.append(0)
                except IndexError:
                    fingers_up.append(0)
                except Exception:
                    fingers_up.append(0)
            
            # Gesture classification - YENÄ°DEN YAPILANDIRILMIÅ KURALLAR
            # up_count = sum(fingers_up) # ArtÄ±k doÄŸrudan fingers_up listesini kullanacaÄŸÄ±z
            
            # Debug print (gerekirse)
            # print(f"ğŸ–ï¸ fingers_up: {fingers_up}")

            # Geometrik koÅŸullar iÃ§in ayrÄ±lmÄ±ÅŸ eÅŸik deÄŸerleri
            EXT_UP_MCP_PIP_THRESH = 0.022  # YukarÄ± uzanma iÃ§in PIP'nin MCP'den ne kadar yukarÄ±da olmasÄ± gerektiÄŸi (0.018 -> 0.022)
            EXT_UP_PIP_TIP_THRESH = 0.022  # YukarÄ± uzanma iÃ§in TIP'nin PIP'den ne kadar yukarÄ±da olmasÄ± gerektiÄŸi (0.018 -> 0.022)
            EXT_DOWN_MCP_PIP_THRESH = 0.022 # AÅŸaÄŸÄ± uzanma iÃ§in PIP'nin MCP'den ne kadar aÅŸaÄŸÄ±da olmasÄ± gerektiÄŸi (DeÄŸiÅŸmedi)
            EXT_DOWN_PIP_TIP_THRESH = 0.022 # AÅŸaÄŸÄ± uzanma iÃ§in TIP'nin PIP'den ne kadar aÅŸaÄŸÄ±da olmasÄ± gerektiÄŸi (DeÄŸiÅŸmedi)

            try:
                # Ä°ÅŸaret ParmaÄŸÄ± (Index Finger) LandmarklarÄ±: MCP=5, PIP=6, TIP=8
                idx_mcp_y = landmarks[5][1]
                idx_pip_y = landmarks[6][1]
                idx_tip_y = landmarks[8][1]

                # Orta Parmak (Middle Finger) LandmarklarÄ±: MCP=9, PIP=10, TIP=12
                mid_mcp_y = landmarks[9][1]
                mid_pip_y = landmarks[10][1]
                mid_tip_y = landmarks[12][1]

                # Ä°ÅŸaret parmaÄŸÄ±nÄ±n aÅŸaÄŸÄ± doÄŸru tam uzanÄ±p uzanmadÄ±ÄŸÄ± (YENÄ° TANIM - AYRI EÅÄ°KLERLE)
                # Tip, PIP'nin altÄ±nda OLMALI; PIP, MCP'nin altÄ±nda OLMALI
                is_idx_ext_down = (idx_tip_y > idx_pip_y + EXT_DOWN_PIP_TIP_THRESH and \
                                   idx_pip_y > idx_mcp_y + EXT_DOWN_MCP_PIP_THRESH)

                # Ä°ÅŸaret parmaÄŸÄ±nÄ±n yukarÄ± doÄŸru tam uzanÄ±p uzanmadÄ±ÄŸÄ± (YENÄ° TANIM - AYRI EÅÄ°KLERLE)
                # Tip, PIP'nin Ã¼stÃ¼nde OLMALI; PIP, MCP'nin Ã¼stÃ¼nde OLMALI
                is_idx_ext_up   = (idx_tip_y < idx_pip_y - EXT_UP_PIP_TIP_THRESH and \
                                   idx_pip_y < idx_mcp_y - EXT_UP_MCP_PIP_THRESH)

                # Orta parmaÄŸÄ±n yukarÄ± doÄŸru tam uzanÄ±p uzanmadÄ±ÄŸÄ± (YENÄ° TANIM - AYRI EÅÄ°KLERLE)
                # Tip, PIP'nin Ã¼stÃ¼nde OLMALI; PIP, MCP'nin Ã¼stÃ¼nde OLMALI
                is_mid_ext_up   = (mid_tip_y < mid_pip_y - EXT_UP_PIP_TIP_THRESH and \
                                   mid_pip_y < mid_mcp_y - EXT_UP_MCP_PIP_THRESH)

                # 1. POINT DOWN KontrolÃ¼
                # Ä°ÅŸaret parmaÄŸÄ± geometrik olarak aÅŸaÄŸÄ±da VE tÃ¼m parmaklar (fingers_up) kapalÄ±ysa
                if is_idx_ext_down and fingers_up == [0,0,0,0,0]:
                    return 'point_down'

                # 2. POINT UP KontrolÃ¼
                # Ä°ÅŸaret parmaÄŸÄ± geometrik olarak yukarÄ±da VE sadece iÅŸaret parmaÄŸÄ± (fingers_up) aÃ§Ä±ksa
                if is_idx_ext_up and (fingers_up == [0,1,0,0,0] or fingers_up == [1,1,0,0,0]):
                    return 'point_up'

                # 3. PEACE KontrolÃ¼
                # Ä°ÅŸaret ve orta parmaklar geometrik olarak yukarÄ±da VE 
                #   a) BaÅŸparmak kapalÄ±, iÅŸaret/orta aÃ§Ä±k, diÄŸerleri kapalÄ± (fingers_up)
                if is_idx_ext_up and is_mid_ext_up and fingers_up == [0,1,1,0,0]:
                    return 'peace'
                #   b) BaÅŸparmak aÃ§Ä±k, iÅŸaret/orta aÃ§Ä±k, diÄŸerleri kapalÄ± (fingers_up)
                if is_idx_ext_up and is_mid_ext_up and fingers_up == [1,1,1,0,0]: # Thumb, Index, Middle up
                    return 'peace'

            except IndexError:
                pass # Landmarkler tam gelmemiÅŸ olabilir, bu durumda aÅŸaÄŸÄ±daki genel kurallar denenebilir
            except Exception as e:
                print(f"âš ï¸ Geometrik kontrollerde hata: {e}")

            # 4. FIST KontrolÃ¼
            # En fazla 1 parmak aÃ§Ä±k (Ã¶r. baÅŸparmak), iÅŸaret parmaÄŸÄ± ne yukarÄ±da ne aÅŸaÄŸÄ±da ise (not is_idx_ext_up and not is_idx_ext_down)
            if sum(fingers_up) <= 1:
                try:
                    idx_mcp_y = landmarks[5][1]
                    idx_pip_y = landmarks[6][1]
                    idx_tip_y = landmarks[8][1]
                    is_idx_ext_down = (idx_tip_y > idx_pip_y + EXT_DOWN_PIP_TIP_THRESH and idx_pip_y > idx_mcp_y + EXT_DOWN_MCP_PIP_THRESH)
                    is_idx_ext_up   = (idx_tip_y < idx_pip_y - EXT_UP_PIP_TIP_THRESH and idx_pip_y < idx_mcp_y - EXT_UP_MCP_PIP_THRESH)
                    if not is_idx_ext_up and not is_idx_ext_down:
                        return 'fist'
                except Exception:
                    return 'fist'
            
            # 5. OPEN HAND KontrolÃ¼
            # TÃ¼m parmaklar (fingers_up) aÃ§Ä±ksa
            if fingers_up == [1,1,1,1,1]:
                 return 'open_hand'
            
            # Eski genel up_count tabanlÄ± open_hand kurallarÄ± kaldÄ±rÄ±ldÄ±.
            # EÄŸer yukarÄ±daki kurallardan hiÃ§biri eÅŸleÅŸmezse, None dÃ¶necek.
            
            return None # HiÃ§bir spesifik hareket bulunamadÄ±
                
        except Exception as e:
            print(f"âŒ Rule-based classification genel hata: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_model(self):
        """Saves trained model to disk"""
        if self.is_trained:
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            model_data = {
                'model': self.model,
                'gesture_classes': self.gesture_classes,
                'feature_names': self.feature_names,
                'is_trained': self.is_trained
            }
            
            with open(self.model_path, 'wb') as f:
                pickle.dump(model_data, f)
                
            print(f"âœ… Model kaydedildi: {self.model_path}")
        
    def load_model(self):
        """Loads trained model from disk"""
        if os.path.exists(self.model_path):
            try:
                with open(self.model_path, 'rb') as f:
                    model_data = pickle.load(f)
                    
                self.model = model_data['model']
                self.gesture_classes = model_data['gesture_classes']
                self.feature_names = model_data['feature_names']
                self.is_trained = model_data['is_trained']
                
                print(f"âœ… Model yÃ¼klendi: {self.model_path}")
                
            except Exception as e:
                print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
                self.is_trained = False
    
    def data_augmentation(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        AYÅENUR'UN GELÄ°ÅTÄ°RECEÄÄ° Ã–ZELLÄ°K:
        Veri artÄ±rma teknikleri
        
        Args:
            X: Ã–zellik matrisi
            y: Etiket dizisi
            
        Returns:
            ArtÄ±rÄ±lmÄ±ÅŸ veri seti
        """
        # TODO: AyÅŸenur implement edecek
        # Rotation, scaling, noise ekleme vb.
        return X, y
    
    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray):
        """
        AYÅENUR'UN GELÄ°ÅTÄ°RECEÄÄ° Ã–ZELLÄ°K:
        Hiperparametre optimizasyonu
        
        Args:
            X: Ã–zellik matrisi
            y: Etiket dizisi
        """
        # TODO: AyÅŸenur implement edecek
        # GridSearchCV veya RandomSearchCV kullanarak optimizasyon
        pass

    def optimize_performance(self):
        """
        Optimizes model performance
        Implements caching and parallel processing
        """
        # TODO: AyÅŸenur implement edecek
        pass

    def update_model(self):
        """
        Updates model with new training data
        Implements online learning
        """
        # TODO: AyÅŸenur implement edecek
        pass

    def export_model(self):
        """
        Exports model for deployment
        Converts to optimized format
        """
        # TODO: AyÅŸenur implement edecek
        pass

    def visualize_features(self):
        """
        Visualizes extracted features
        Creates feature importance plots
        """
        # TODO: AyÅŸenur implement edecek
        pass

    def analyze_performance(self):
        """
        Analyzes model performance
        Generates performance reports
        """
        # TODO: AyÅŸenur implement edecek
        pass

    def print_feature_status(self):
        """Prints status of features to be implemented"""
        print("ğŸ“ Features to be implemented:")
        # TODO: AyÅŸenur implement edecek
        pass


# Test kodu
if __name__ == "__main__":
    # Basit test
    classifier = GestureClassifier()
    
    print("âœ… GestureClassifier sÄ±nÄ±fÄ± hazÄ±r!")
    print("ğŸ“ AyÅŸenur'un implement edeceÄŸi Ã¶zellikler:")
    print("   - calculate_finger_angles()")
    print("   - collect_training_data()")
    print("   - data_augmentation()")
    print("   - optimize_hyperparameters()")
    print("   - GeliÅŸmiÅŸ feature engineering")
    
    # Dummy veri ile test
    dummy_landmarks = np.random.rand(63)
    features = classifier.extract_advanced_features(dummy_landmarks)
    print(f"ğŸ“Š Ã‡Ä±karÄ±lan Ã¶zellik sayÄ±sÄ±: {len(features)}")
    print(f"ğŸ”§ Model eÄŸitim durumu: {classifier.is_trained}") 